{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### PyTorch has emerged as a major contender in the race to be the king of deep learning frameworks.\n",
    "\n",
    "In this notebook I will go over some regular snippets and techniques of it."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute basic gradients from the sample tensors using PyTorch\n",
    "\n",
    "#### First some basics of Pytorch here\n",
    "\n",
    "**Autograd**: This class is an engine to calculate derivatives (Jacobian-vector product to be more precise). It records a graph of all the operations performed on a gradient enabled tensor and creates an acyclic graph called the dynamic computational graph. The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule.\n",
    "\n",
    "A Variable class wraps a tensor. You can access this tensor by calling `.data` attribute of a Variable.\n",
    "\n",
    "The Variable also stores the gradient of a scalar quantity (say, loss) with respect to the parameter it holds. This gradient can be accessed by calling the `.grad` attribute. This is basically the gradient computed up to this particular node, and the gradient of the every subsequent node, can be computed by multiplying the edge weight with the gradient computed at the node just before it.\n",
    "\n",
    "The third attribute a Variable holds is a grad_fn, a Function object which created the variable.\n",
    "\n",
    "**Variable**: The Variable, just like a Tensor is a class that is used to hold data. It differs, however, in the way it’s meant to be used. Variables are specifically tailored to hold values which change during training of a neural network, i.e. the learnable paramaters of our network. Tensors on the other hand are used to store values that are not to be learned. For example, a Tensor maybe used to store the values of the loss generated by each example.\n",
    "\n",
    "Every **variable** object has several members one of them is **grad**:\n",
    "\n",
    "**grad**: grad holds the value of gradient. If requires_grad is False it will hold a None value. Even if requires_grad is True, it will hold a None value unless .backward() function is called from some other node. For example, if you call out.backward() for some variable out that involved x in its calculations then x.grad will hold ∂out/∂x.\n",
    "\n",
    "**Backward() function**\n",
    "Backward is the function which actually calculates the gradient by passing it’s argument (1x1 unit tensor by default) through the backward graph all the way up to every leaf node traceable from the calling root tensor. The calculated gradients are then stored in .grad of every leaf node. Remember, the backward graph is already made dynamically during the forward pass. Backward function only calculates the gradient using the already made graph and stores them in leaf nodes."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "# . On setting .requires_grad = True they start forming a backward graph\n",
    "# that tracks every operation applied on them to calculate the gradients\n",
    "# using something called a dynamic computation graph (DCG)\n",
    "# When you finish your computation you can call .backward() and have\n",
    "# all the gradients computed automatically. The gradient for this tensor\n",
    "# will be accumulated into .grad attribute.\n",
    "\n",
    "# Now create an array of data.\n",
    "# By PyTorch’s design, gradients can only be calculated\n",
    "# for floating point tensors which is why I’ve created a float type\n",
    "# array before making it a gradient enabled PyTorch tensor\n",
    "x_data = [11.0, 22.0, 33.0]\n",
    "y_data = [21.0, 14.0, 64.0]\n",
    "\n",
    "def loss_function(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# Now running the training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss_function(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad\n",
    "\n",
    "        # Manually set the gradient to zero after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "        print('progress: ', epoch, l.data[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tgrad:  11.0 21.0 tensor(-220.)\n",
      "progress:  0 tensor(100.)\n",
      "\tgrad:  22.0 14.0 tensor(2481.6001)\n",
      "progress:  0 tensor(3180.9602)\n",
      "\tgrad:  33.0 64.0 tensor(-51303.6484)\n",
      "progress:  0 tensor(604238.8125)\n",
      "\tgrad:  11.0 21.0 tensor(118461.7578)\n",
      "progress:  1 tensor(28994192.)\n",
      "\tgrad:  22.0 14.0 tensor(-671630.6875)\n",
      "progress:  1 tensor(2.3300e+08)\n",
      "\tgrad:  33.0 64.0 tensor(13114108.)\n",
      "progress:  1 tensor(3.9481e+10)\n",
      "\tgrad:  11.0 21.0 tensor(-30279010.)\n",
      "progress:  2 tensor(1.8943e+12)\n",
      "\tgrad:  22.0 14.0 tensor(1.7199e+08)\n",
      "progress:  2 tensor(1.5279e+13)\n",
      "\tgrad:  33.0 64.0 tensor(-3.3589e+09)\n",
      "progress:  2 tensor(2.5900e+15)\n",
      "\tgrad:  11.0 21.0 tensor(7.7553e+09)\n",
      "progress:  3 tensor(1.2427e+17)\n",
      "\tgrad:  22.0 14.0 tensor(-4.4050e+10)\n",
      "progress:  3 tensor(1.0023e+18)\n",
      "\tgrad:  33.0 64.0 tensor(8.6030e+11)\n",
      "progress:  3 tensor(1.6991e+20)\n",
      "\tgrad:  11.0 21.0 tensor(-1.9863e+12)\n",
      "progress:  4 tensor(8.1519e+21)\n",
      "\tgrad:  22.0 14.0 tensor(1.1282e+13)\n",
      "progress:  4 tensor(6.5750e+22)\n",
      "\tgrad:  33.0 64.0 tensor(-2.2034e+14)\n",
      "progress:  4 tensor(1.1146e+25)\n",
      "\tgrad:  11.0 21.0 tensor(5.0875e+14)\n",
      "progress:  5 tensor(5.3477e+26)\n",
      "\tgrad:  22.0 14.0 tensor(-2.8897e+15)\n",
      "progress:  5 tensor(4.3132e+27)\n",
      "\tgrad:  33.0 64.0 tensor(5.6436e+16)\n",
      "progress:  5 tensor(7.3118e+29)\n",
      "\tgrad:  11.0 21.0 tensor(-1.3030e+17)\n",
      "progress:  6 tensor(3.5081e+31)\n",
      "\tgrad:  22.0 14.0 tensor(7.4013e+17)\n",
      "progress:  6 tensor(2.8295e+32)\n",
      "\tgrad:  33.0 64.0 tensor(-1.4455e+19)\n",
      "progress:  6 tensor(4.7966e+34)\n",
      "\tgrad:  11.0 21.0 tensor(3.3374e+19)\n",
      "progress:  7 tensor(2.3013e+36)\n",
      "\tgrad:  22.0 14.0 tensor(-1.8957e+20)\n",
      "progress:  7 tensor(1.8562e+37)\n",
      "\tgrad:  33.0 64.0 tensor(3.7022e+21)\n",
      "progress:  7 tensor(inf)\n",
      "\tgrad:  11.0 21.0 tensor(-8.5480e+21)\n",
      "progress:  8 tensor(inf)\n",
      "\tgrad:  22.0 14.0 tensor(4.8553e+22)\n",
      "progress:  8 tensor(inf)\n",
      "\tgrad:  33.0 64.0 tensor(-9.4824e+23)\n",
      "progress:  8 tensor(inf)\n",
      "\tgrad:  11.0 21.0 tensor(2.1894e+24)\n",
      "progress:  9 tensor(inf)\n",
      "\tgrad:  22.0 14.0 tensor(-1.2436e+25)\n",
      "progress:  9 tensor(inf)\n",
      "\tgrad:  33.0 64.0 tensor(2.4287e+26)\n",
      "progress:  9 tensor(inf)\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Weight initialization is an important task in training a neural network,\n",
    "whether its a convolutional neural network\n",
    "(CNN), a deep neural network (DNN), and a recurrent neural network\n",
    "(RNN). Lets some examples of initializing the weights.\n",
    "\n",
    "\n",
    "Weight initialization can be done by using various methods, including\n",
    "random weight initialization.\n",
    "Weight initialization based on a distribution\n",
    "is done using\n",
    "- Uniform distribution,\n",
    "- Bernoulli distribution,\n",
    "- Multinomial distribution, and normal distribution.\n",
    "\n",
    "To execute a neural network, a set of initial weights needs to be passed to\n",
    "the backpropagation layer to compute the loss function (and hence, the\n",
    "accuracy can be calculated). The selection of a method depends on the\n",
    "data type, the task, and the optimization required for the model."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bernoulli Distribution is a random experiment that has only two outcomes (usually called a “Success” or a “Failure”). It is best used when we have two outcomes of a given event. Its considered as the discrete\n",
    "probability distribution, which has two possible outcomes. If the event happens, then the value is 1, and if the event does not happen, then the value is 0.\n",
    "\n",
    "For discrete probability distribution, we calculate probability mass\n",
    "function instead of probability density function. The probability mass\n",
    "function looks like the following formula.\n",
    "\n",
    "![](https://i.imgur.com/bz2dWtc.png)\n",
    "\n",
    "From the Bernoulli distribution, we create sample tensors by considering the uniform distribution of size 4 and 4 in a matrix format, as follows.\n",
    "\n",
    " Specifically, `torch.bernoulli()` samples from the distribution and returns a binary value (i.e. either 0 or 1). Here, it returns 1 with probability p and return 0 with probability 1-p.\n",
    "\n",
    "```python\n",
    "torch.bernoulli(input, *, generator=None, out=None)\n",
    "```\n",
    "It draws binary random numbers (0 or 1) from a Bernoulli distribution.\n",
    "\n",
    "Syntax\n",
    "\n",
    "```python\n",
    "torch.bernoulli(input, *, generator=None, out=None) → Tensor\n",
    "```\n",
    "\n",
    "Parameters :\n",
    "\n",
    "input (Tensor) – the input tensor of probability values for the Bernoulli distribution\n",
    "\n",
    "generator (torch.Generator, optional) – a pseudorandom number generator for sampling\n",
    "\n",
    "out (Tensor, optional) – out tensor only has values 0 or 1 and is of the same shape as input.\n",
    "\n",
    "The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range:\n",
    "\n",
    "0 <= input_i <=1\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "torch.bernoulli(torch.Tensor(4, 4).uniform_(0, 1))\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generation of sample random values from a multinomial distribution\n",
    "\n",
    "Note the syntax of multinomial function from official doc\n",
    "\n",
    "```python\n",
    "torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) → LongTensor\n",
    "```\n",
    "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sample_tensor = torch.Tensor([10, 10, 13, 10, 34,45,65,67,87,89,87,34])\n",
    "torch.multinomial(torch.tensor([10., 10., 13., 10., 34., 45., 65., 67., 87., 89., 87., 34.]), 3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 8, 10, 11])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sampling from multinomial distribution with a replacement returns the tensors’ index values."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "torch.multinomial(torch.tensor([10., 10., 13., 10., 34., 45., 65., 67., 87., 89., 87., 34.]), 5, replacement=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([10,  8,  8,  0,  8])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now, the weight initialization from the normal distribution, which is also a method\n",
    "that is used in fitting a neural network, fitting a deep neural network, and\n",
    "CNN and RNN. Let’s have a look at the process of creating a set of random\n",
    "weights generated from a normal distribution.\n",
    "\n",
    "Syntax\n",
    "\n",
    "```python\n",
    "torch.normal(mean, std, *, generator=None, out=None) → Tensor\n",
    "```\n",
    "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.\n",
    "\n",
    "The mean is a tensor with the mean of each output element’s normal distribution\n",
    "\n",
    "The std is a tensor with the standard deviation of each output element’s normal distribution\n",
    "\n",
    "The shapes of mean and std don’t need to match, but the total number of elements in each tensor need to be the same."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "torch.normal(mean=torch.arange(1., 11), std=torch.arange(1, 0, -0.1))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.4350,  1.3608,  2.3142,  4.1479,  4.6382,  5.9646,  6.9279,  7.9011,\n",
       "         8.9942, 10.0591])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "torch.normal(mean=0.5, std=torch.arange(1.,6.))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.3024,  1.6342,  3.4563, -2.8438,  3.4015])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "torch.normal(mean=0.5, std=torch.arange(0.2, 0.6))\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.5592])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variable in PyTorch and its defined? What is a random variable in PyTorch?\n",
    "\n",
    "In PyTorch, the algorithms are represented as a computational graph.\n",
    "\n",
    "A variable is considered as a representation around the tensor object,\n",
    "corresponding gradients (slope of the function), and a reference to the function from where it was\n",
    "created. \n",
    "\n",
    "The slope of the function can be computed by the derivative of the\n",
    "function with respect to the parameters that are present in the function.\n",
    "\n",
    "Basically, a PyTorch variable is a node in a computational graph, which\n",
    "stores data and gradients. When training a neural network model, after\n",
    "each iteration, we need to compute the gradient of the loss function with\n",
    "respect to the parameters of the model, such as weights and biases. After\n",
    "that, we usually update the weights using the gradient descent algorithm.\n",
    "\n",
    "Below Figure explains how the linear regression equation is deployed under\n",
    "the hood using a neural network model in the PyTorch framework.\n",
    "In a computational graph structure, the sequencing and ordering\n",
    "of tasks is very important. The one-dimensional tensors are X, Y, W,\n",
    "and alpha. The direction of the arrows change when we\n",
    "implement backpropagation to update the weights to match with Y, so that\n",
    "the error or loss function between Y and predicted Y can be minimized.\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/6JOtOGb.png)\n",
    "\n",
    "#### Lets see and example\n",
    "\n",
    "An example of how a variable is used to create a computational graph is\n",
    "displayed in the following script. There are three variable objects around\n",
    "tensors— x1, x2, and x3—with random points generated from a = 12 and\n",
    "b = 23. The graph computation involves only multiplication and addition,\n",
    "and the final result with the gradient is shown.\n",
    "\n",
    "The partial derivative of the loss function with respect to the weights\n",
    "and biases in a neural network model is achieved in PyTorch using the\n",
    "Autograd module. Variables are specifically designed to hold the changed\n",
    "values while running a backpropagation in a neural network model when\n",
    "the parameters of the model change. The variable type is just a wrapper\n",
    "around the tensor. It has three properties: data, grad, and function.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch.autograd import Variable\n",
    "Variable(torch.ones(2,2), requires_grad=True)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "a, b = 12, 23\n",
    "x1 = Variable(torch.randn(a, b), requires_grad=True )\n",
    "x2 = Variable(torch.randn(a,b), requires_grad=True)\n",
    "x3 = Variable(torch.randn(a,b), requires_grad=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "c = x1 * x2\n",
    "d = a + x3\n",
    "e = torch.sum(d)\n",
    "\n",
    "e.backward()\n",
    "\n",
    "print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(3305.0916, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "x1.data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.5805, -1.6211, -0.0220,  0.1063,  0.6003,  1.1850,  0.5650, -0.4607,\n",
       "          1.0795, -0.2618, -0.5876,  0.9575, -0.0534,  1.2101,  1.3989, -0.0707,\n",
       "         -0.6653, -0.6698, -0.5093, -0.1171, -0.0559, -0.6459,  0.7618],\n",
       "        [-1.6172, -1.4776, -1.3159, -0.7741,  0.2111,  1.6640,  0.1075,  0.8687,\n",
       "          0.0398,  0.4285,  0.1697,  0.4668, -0.3620,  0.7425,  0.4021,  0.4445,\n",
       "          1.7736, -0.5256,  1.9984,  1.0971, -0.2418, -0.9940,  0.4842],\n",
       "        [-1.2023,  0.3436,  0.0941, -1.2934, -1.0873,  0.4688,  1.3987, -1.3774,\n",
       "          0.1682,  0.8509, -0.7933,  0.4016, -0.2627, -1.2704, -0.6581, -0.6152,\n",
       "         -1.7383, -0.7637,  0.4793, -0.1167, -0.4131,  0.6578, -1.4892],\n",
       "        [-0.5291,  0.2822, -0.2390, -0.5625, -1.0537, -0.3463, -1.3984,  0.7116,\n",
       "          0.6512,  1.7050,  0.7495,  0.5477, -0.0778, -0.1877,  0.5774, -0.5838,\n",
       "         -0.5228, -1.5167,  1.2894, -0.7242,  0.3746,  1.9546, -0.7291],\n",
       "        [-1.2382, -1.7610,  0.3786, -0.7586, -1.4637, -0.3829,  0.0892,  0.7641,\n",
       "          1.8610, -0.5283, -0.0091, -0.5824, -0.3048, -0.8154, -0.1217,  0.0969,\n",
       "          0.0740, -0.0826,  0.3246,  3.0313, -0.2595, -0.6610,  0.3461],\n",
       "        [ 1.0768, -1.3195,  0.1504,  0.5244, -0.3016, -1.2934, -0.5468,  0.0757,\n",
       "          1.2632,  0.2418, -0.6383,  0.6609, -0.6916,  0.1975, -0.6954, -0.5032,\n",
       "         -1.3927,  2.4187, -0.4001, -1.0784, -0.7933, -0.1648,  2.2433],\n",
       "        [-0.8136,  0.5206,  0.8296,  1.2058,  0.4634,  0.5764,  0.1348,  2.5370,\n",
       "          0.6076, -0.2817, -1.0641, -0.3371,  1.8157,  0.7144, -0.4686,  0.3910,\n",
       "         -0.7487, -0.1606,  1.1036, -1.2154,  0.8614,  0.8918,  1.7860],\n",
       "        [-0.3962, -0.0697,  1.1503, -0.0982, -0.0405,  0.5280,  1.0479, -0.7082,\n",
       "         -0.0104, -0.5230, -1.0286,  0.2731, -1.0168, -0.6152, -1.4100,  1.9873,\n",
       "          0.1301, -0.3312,  0.1067,  1.6253,  0.7551,  0.4201,  2.0290],\n",
       "        [-1.5014,  0.8058,  0.6221, -0.3641, -2.2269, -0.0594,  0.1669,  0.3005,\n",
       "         -0.6035, -0.3602, -1.2342, -2.0900,  0.8931, -1.2924,  2.1126, -0.1741,\n",
       "         -0.6868, -0.5075, -0.8756, -0.0270, -0.0593,  0.5806,  0.4048],\n",
       "        [-0.8873,  2.0300, -1.2269,  0.8868, -2.3819,  0.4661, -0.2276,  1.2045,\n",
       "         -1.4276,  1.4913, -0.0956,  1.4279,  0.7564,  1.6821, -1.1181,  1.1147,\n",
       "         -1.0508, -0.5738, -1.7338, -1.2684, -0.5112,  0.3152, -0.8182],\n",
       "        [ 0.0391, -1.4265, -1.8347, -0.5455,  1.2514,  0.2143,  0.5606, -2.4089,\n",
       "          1.1340,  0.8610, -0.3444,  0.8686,  0.4849,  0.8121, -0.6801,  0.8116,\n",
       "         -0.7284, -0.2682,  1.0829,  0.9809, -0.9968,  1.2614,  0.4990],\n",
       "        [ 0.0243, -1.3016,  2.0229, -0.4952,  0.7751, -0.4203,  0.1120,  1.3910,\n",
       "          0.1010,  0.3802,  0.2373, -0.0958,  0.8454,  0.8361, -0.7300,  0.6520,\n",
       "          0.2578,  1.4234,  1.2955, -0.5175, -0.3065,  0.8135, -0.8025]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x2.data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.5597, -0.1538,  0.1115,  1.0113, -0.2525, -2.3110,  2.0029, -0.2659,\n",
       "         -0.7239, -0.5386,  1.1574,  0.7259,  1.2875, -0.1156,  0.4462, -0.0466,\n",
       "         -0.2411, -0.3121,  0.3035, -0.0231,  0.1575,  0.0137,  1.4618],\n",
       "        [-2.6362, -0.1665, -0.3811,  1.4514,  0.9064, -0.7623,  1.9534,  1.1944,\n",
       "          0.6413, -0.7048, -0.4018, -0.5417, -0.9185,  2.3222, -0.8024,  0.3821,\n",
       "          0.1069, -0.7797,  1.8992,  0.1724, -1.9079,  1.3650,  0.1518],\n",
       "        [-1.8273,  0.4821, -0.0423,  1.3044,  0.7008,  0.9906, -1.9159, -0.0876,\n",
       "          0.5237, -0.2249, -0.8327,  0.3556,  0.4029,  0.9445, -1.0837, -0.9799,\n",
       "         -0.5934, -1.1064, -0.9814,  1.7679, -0.3688,  1.0304,  0.7488],\n",
       "        [ 0.0643, -2.3823,  0.4109,  1.2474,  0.1031, -1.2063, -0.4925, -1.0155,\n",
       "         -0.0631,  1.8255,  0.7474, -0.3815,  0.4656,  0.2265, -0.0691,  0.9685,\n",
       "         -0.7942, -0.0685, -0.6331,  0.5485,  0.1820, -0.8391,  0.0730],\n",
       "        [-1.3950,  0.6110,  1.1048,  0.0879,  0.3168, -0.4545,  0.3014, -0.3656,\n",
       "         -0.3004, -1.7698,  0.5465, -0.7211, -1.5699, -0.7370,  0.1751,  0.4781,\n",
       "         -0.2758, -0.2016,  0.4742,  0.3870,  2.3308,  0.8071,  0.2119],\n",
       "        [-0.3988,  1.0616,  0.4288, -0.7707,  0.2564, -0.9324, -0.1350,  0.0743,\n",
       "          1.1915, -1.4410, -0.1280, -0.4790,  1.7953,  0.7974,  1.8187, -0.4806,\n",
       "          1.3855, -1.3806,  0.2214,  0.4298,  0.1070,  1.7714,  0.7836],\n",
       "        [-0.2928,  0.0497,  1.2426, -0.6219,  1.1674,  0.5411,  0.9838, -0.6614,\n",
       "         -1.4380, -1.7852,  0.2678, -0.0728, -0.8505, -0.0910, -0.9448,  1.4261,\n",
       "          1.5484,  0.8422,  0.7240,  0.2657,  0.1112, -1.3199,  1.0552],\n",
       "        [ 0.7457,  1.2480,  1.3011,  0.0106,  0.9646,  2.2751, -0.5781,  0.8418,\n",
       "         -1.5260,  0.5751,  0.8267,  1.3890,  0.1611, -0.5609,  1.1245,  0.3115,\n",
       "         -0.4470, -0.9123, -1.0708,  0.8608, -0.1973,  1.7204,  0.4089],\n",
       "        [-0.5734, -0.0609,  0.8876,  0.3335, -0.4824,  1.1975,  1.4111,  0.4554,\n",
       "         -0.2152, -0.1889, -0.7569,  1.2023,  1.0419, -0.6362, -0.1977, -1.2354,\n",
       "         -0.1121,  0.2279,  1.4577,  0.0826, -1.0527, -0.9415, -0.9032],\n",
       "        [-2.0864, -1.5390,  0.2146,  1.1643,  0.2354,  0.7206, -0.1196, -0.2624,\n",
       "         -1.5582, -0.9898,  0.4284, -1.6353,  0.1617, -0.6581, -1.1890, -0.5013,\n",
       "         -0.6289,  0.1845,  0.7425, -1.9128,  0.3577,  0.1096, -1.5688],\n",
       "        [ 0.1151, -0.6778, -0.9438,  1.1064, -2.3674,  0.4705, -0.1786,  1.8136,\n",
       "          0.2578,  0.7418, -0.5132,  0.7965, -0.1763,  0.3553,  1.6404,  0.0936,\n",
       "         -0.5464,  0.6319, -0.5290, -0.2674,  0.0963,  1.9537, -0.0928],\n",
       "        [ 1.2013,  0.1046,  0.4224,  1.6402,  1.3527, -0.4571, -0.3540, -0.6693,\n",
       "          1.4569,  0.3025, -1.2501,  0.4453, -0.0501, -0.3637, -1.0339, -2.6386,\n",
       "         -0.3890,  1.0842, -0.3176, -1.0246,  0.7185,  0.5311,  0.5368]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "x3.data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.5732e+00, -1.8919e+00, -9.5864e-01,  1.0764e+00, -1.1491e+00,\n",
       "          1.2582e+00,  1.3774e+00, -7.9841e-01,  4.1802e-01,  1.3354e+00,\n",
       "         -2.2857e+00,  1.2406e+00,  3.2930e-01,  2.7476e-01, -7.1076e-01,\n",
       "         -7.8910e-01, -1.3080e-01,  4.5392e-01,  8.6285e-01,  8.0801e-01,\n",
       "          5.4228e-01,  1.6918e-01, -3.9944e-01],\n",
       "        [ 2.3779e-01, -1.1615e-01, -5.4451e-01,  1.4247e+00,  7.1588e-01,\n",
       "          1.5356e+00,  4.4198e-03,  3.7880e-01, -7.9946e-01,  3.4540e-01,\n",
       "          1.4109e+00,  1.4679e-02, -1.4088e+00,  6.4824e-01, -8.7981e-01,\n",
       "         -1.8292e+00,  1.2675e+00,  4.0991e-01, -1.9237e+00,  1.0276e+00,\n",
       "         -5.1640e-01, -3.6208e-01,  1.9239e+00],\n",
       "        [-5.2775e-01,  1.9665e-02, -1.2310e+00, -2.2846e-01,  5.6001e-01,\n",
       "          1.5126e-01,  3.8631e+00,  7.2139e-01, -2.4427e+00, -3.1431e-01,\n",
       "          1.0223e+00,  1.8373e+00, -1.5903e+00,  8.1811e-01, -5.5065e-01,\n",
       "          1.9191e-01,  4.3877e-01,  9.3067e-01,  7.9798e-02,  1.3302e+00,\n",
       "         -4.7027e-01, -7.6023e-01,  3.8975e-03],\n",
       "        [ 1.4254e-01,  1.6209e+00, -8.1097e-01,  7.4380e-01,  2.7857e-01,\n",
       "         -1.9239e-01,  1.1661e+00, -2.4421e-01, -1.8524e+00, -1.4898e+00,\n",
       "         -6.4976e-01,  8.9743e-01, -1.5196e+00,  2.5662e-01,  1.5001e+00,\n",
       "         -3.3304e-01,  1.5898e+00, -3.6900e-02,  9.2748e-02, -1.5498e+00,\n",
       "          1.0885e+00,  1.6632e-01, -3.6470e-01],\n",
       "        [ 3.8703e-01,  1.0557e+00, -3.3043e-02,  6.3061e-01, -1.7945e-01,\n",
       "         -8.2657e-01,  3.6617e-01, -2.2861e+00,  4.2889e-01, -1.0951e+00,\n",
       "          1.0792e+00,  1.1225e+00, -1.0035e+00,  1.0793e+00, -1.8762e+00,\n",
       "          6.0121e-01, -1.7932e+00,  9.9093e-01, -2.2152e+00, -1.9373e+00,\n",
       "         -3.2717e-01,  1.6063e-01, -9.1582e-01],\n",
       "        [-2.2265e+00, -7.3184e-01, -3.3989e-01, -6.1185e-01, -9.4512e-01,\n",
       "         -6.4762e-01,  1.7454e+00,  1.8969e+00,  8.0583e-01,  1.4578e+00,\n",
       "          6.2155e-01,  1.5927e+00, -2.3608e+00,  7.6854e-01, -2.3288e+00,\n",
       "         -7.5004e-01,  1.3734e+00, -3.5990e-01, -7.9325e-01,  6.5257e-01,\n",
       "          2.9716e-01, -4.9513e-01,  8.1275e-01],\n",
       "        [ 6.2067e-01, -1.6523e+00, -7.7669e-01,  1.0962e+00,  4.0938e-02,\n",
       "         -4.9778e-01,  8.5743e-01,  1.0493e+00,  2.1242e+00,  1.0939e-01,\n",
       "          2.7584e-01,  4.0059e-01, -1.5061e+00,  7.1122e-01,  3.0561e-01,\n",
       "          1.9249e-01,  1.2393e+00,  9.3453e-02, -1.6932e-01,  1.6625e+00,\n",
       "         -3.1057e-01,  1.1793e+00, -1.4627e+00],\n",
       "        [ 1.0047e+00, -6.1973e-01,  1.1868e-03,  1.2928e+00,  1.4799e+00,\n",
       "         -3.7539e-01, -5.3877e-01, -1.6967e-01, -2.8008e-01,  2.9274e-01,\n",
       "         -7.8095e-01, -2.7560e-01, -9.7597e-03,  5.5733e-01,  3.4230e-01,\n",
       "         -1.3730e+00, -2.8017e-01, -1.9276e-01,  1.2007e+00,  6.4560e-01,\n",
       "         -1.9605e-01,  3.0383e-01,  4.1425e-01],\n",
       "        [ 5.0648e-01, -1.0299e+00,  1.4668e+00, -9.1976e-01,  9.1149e-01,\n",
       "         -1.8925e+00, -1.9236e+00,  2.0070e-01, -6.6036e-01, -1.4868e+00,\n",
       "         -8.9918e-01, -1.5853e+00, -1.2040e+00,  7.5543e-01,  4.5511e-02,\n",
       "         -4.8197e-01, -4.3369e-01,  9.8977e-01, -7.2694e-01,  9.3133e-02,\n",
       "         -6.1525e-01,  1.4288e-01,  1.5459e+00],\n",
       "        [ 1.2734e+00, -8.0012e-01, -1.5938e+00, -1.2735e+00,  1.2898e+00,\n",
       "         -5.6168e-01, -1.7532e+00,  2.9283e-01, -1.5175e+00,  1.0171e-01,\n",
       "          3.5545e-01, -2.8807e-01,  5.2614e-02,  7.9120e-01, -2.6883e-01,\n",
       "         -6.6671e-01,  3.9932e-01,  7.9850e-01, -3.2418e-01, -2.3412e+00,\n",
       "         -2.4240e-01, -1.6638e-01, -1.5055e+00],\n",
       "        [-1.4611e+00, -1.2706e-01, -3.9489e-01,  1.1258e+00,  4.0897e-01,\n",
       "         -6.5174e-01,  1.1028e+00,  1.8287e+00, -1.0853e+00,  7.9457e-01,\n",
       "          1.3147e-02, -1.4920e+00,  4.8084e-01, -1.8590e+00, -5.9708e-02,\n",
       "         -2.0241e+00, -1.4736e+00, -2.6765e-01, -9.6613e-01, -1.8482e+00,\n",
       "          7.0701e-01,  1.6574e+00,  6.3316e-01],\n",
       "        [ 2.4446e+00, -2.9312e-01,  5.2989e-01, -7.6635e-01,  6.3581e-01,\n",
       "         -1.3485e-01,  6.7413e-01, -4.3964e-01,  4.6778e-02, -4.2667e-01,\n",
       "          8.4801e-02, -4.3630e-01,  2.5547e-01,  1.8158e-01,  1.1429e+00,\n",
       "         -4.0509e-01,  1.3097e-01,  1.3979e+00,  8.4871e-02, -6.6176e-01,\n",
       "         -1.2087e+00, -3.3199e-01, -1.0362e-01]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How do we set up a loss function and optimize it ? \n",
    "\n",
    "Choosing the right loss function increases the chances of model convergence. \n",
    "\n",
    "we use another tensor as the update variable, and introduce\n",
    "the tensors to the sample model and compute the error or loss. Then we\n",
    "compute the rate of change in the loss function to measure the choice of\n",
    "loss function in model convergence.\n",
    "\n",
    "In the following example, t_c and t_u are two tensors. This can be\n",
    "constructed from any NumPy array.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "torch.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "torch.tensor"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function _VariableFunctionsClass.tensor>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The sample model is just a linear equation to make the calculation\n",
    "happen and the loss function defined if the mean square error (MSE)\n",
    "shown next. For now, this is just a simple linear equation\n",
    "computation.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#height of people\n",
    "t_c = torch.tensor([58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0])\n",
    "\n",
    "#weight of people\n",
    "t_u = torch.tensor([115.0, 117.0, 120.0, 123.0, 126.0, 129.0, 132.0, 135.0, 139.0, 142.0, 146.0, 150.0, 154.0, 159.0,164.0])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s now define the model. The w parameter is the weight tensor,\n",
    "which is multiplied with the t_u tensor. The result is added with a constant\n",
    "tensor, b, and the loss function chosen is a custom-built one; it is also available in PyTorch. \n",
    "\n",
    "In the following example, t_u is the tensor used, t_p\n",
    "is the tensor predicted, and t_c is the precomputed tensor, with which the\n",
    "predicted tensor needs to be compared to calculate the loss function.\n",
    "\n",
    "The formula $$w * t_u + b$$ is the linear equation representation of a\n",
    "tensor-based computation.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "  \n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "  \n",
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([115., 117., 120., 123., 126., 129., 132., 135., 139., 142., 146., 150.,\n",
       "        154., 159., 164.])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(5259.7334)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The initial loss value is 5259.7334, which is too high because of the\n",
    "initial round of weights chosen. The error in the first round of iteration\n",
    "is backpropagated to reduce the errors in the second round, for which\n",
    "the initial set of weights needs to be updated. Therefore, the rate of\n",
    "change in the loss function is essential in updating the weights in the\n",
    "estimation process.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, \n",
    "                                       w + delta, b), \n",
    "                                 t_c) - loss_fn(model(t_u, w - delta, b), \n",
    "                                                t_c)) / (2.0 * delta)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two parameters to update the rate of loss function: the\n",
    "learning rate at the current iteration and the learning rate at the previous\n",
    "iteration. If the delta between the two iterations exceeds a certain\n",
    "threshold, then the weight tensor needs to be updated, else model\n",
    "convergence could happen. The preceding script shows the delta and\n",
    "learning rate values. Currently, these are static values that the user has the\n",
    "option to change.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - \n",
    "                         loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "\n",
    "b = b - learning_rate * loss_rate_of_change_b\n",
    "\n",
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([544.])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is how a simple mean square loss function works in a two-­\n",
    "dimensional tensor example, with a tensor size of 10,5.\n",
    "Let’s look at the following example. The MSELoss function is within the\n",
    "neural network module of PyTorch.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from torch import nn\n",
    "loss = nn.MSELoss()\n",
    "input = torch.randn(10, 5, requires_grad=True)\n",
    "target = torch.randn(10, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we look at the gradient calculation that is used for\n",
    "backpropagation, it is shown as MSELoss.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "output.grad_fn"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<MseLossBackward at 0x7fa4f40b4280>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor differentiation and its relevance in computational graph execution using the PyTorch framework\n",
    "\n",
    "The computational graph network is represented by nodes and connected\n",
    "through functions. There are two different kinds of nodes: dependent and\n",
    "independent. Dependent nodes are waiting for results from other nodes\n",
    "to process the input. Independent nodes are connected and are either\n",
    "constants or the results. Tensor differentiation is an efficient method to\n",
    "perform computation in a computational graph environment.\n",
    "\n",
    "In a computational graph, tensor differentiation is very effective because\n",
    "the tensors can be computed as parallel nodes, multiprocess nodes, or\n",
    "multithreading nodes. The major deep learning and neural computation\n",
    "frameworks include this tensor differentiation.\n",
    "Autograd is the function that helps perform tensor differentiation,\n",
    "which means calculating the gradients or slope of the error function,\n",
    "and backpropagating errors through the neural network to fine-tune the\n",
    "weights and biases. Through the learning rate and iteration, it tries to\n",
    "reduce the error value or loss function.\n",
    "To apply tensor differentiation, the nn.backward() method needs to\n",
    "be applied. Let’s take an example and see how the error gradients are\n",
    "backpropagated. To update the curve of the loss function, or to find where\n",
    "the shape of the loss function is minimum and in which direction it is\n",
    "moving, a derivative calculation is required. Tensor differentiation is a way\n",
    "to compute the slope of the function in a computational graph.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Make a sample tensor x, for which automatic gradient calculation needs to happen.\n",
    "x = Variable(torch.ones(4, 4) * 12.5, requires_grad=True)\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[12.5000, 12.5000, 12.5000, 12.5000],\n",
       "        [12.5000, 12.5000, 12.5000, 12.5000],\n",
       "        [12.5000, 12.5000, 12.5000, 12.5000],\n",
       "        [12.5000, 12.5000, 12.5000, 12.5000]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#  Create a linear function fn that is created using the x variable.\n",
    "fn = 2 * (x * x) + 5 * x + 6\n",
    "\n",
    "\n",
    "#  Using the backward function, we can perform a backpropagation calculation. \n",
    "fn.backward(torch.ones(4,4))\n",
    "\n",
    "# The .grad() function holds the final output from the tensor differentiation.\n",
    "x.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[55., 55., 55., 55.],\n",
       "        [55., 55., 55., 55.],\n",
       "        [55., 55., 55., 55.],\n",
       "        [55., 55., 55., 55.]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}